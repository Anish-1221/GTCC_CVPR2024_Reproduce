# Comprehensive Codebase Analysis Report: GTCC_CVPR2024

**Generated by Claude Code**
**Date: January 31, 2026**

---

## 1. Project Overview and Purpose

### Paper Title
**"Learning to Predict Activity Progress by Self-Supervised Video Alignment"**
Authors: Gerard Donahue and Ehsan Elhamifar (Northeastern University)
Published at: CVPR 2024

### Core Problem
The project addresses **self-supervised video alignment and online activity progress prediction** for procedural videos, including both:
- **Monotonic videos**: Actions occur in a fixed sequential order (e.g., pouring liquid, athletic maneuvers)
- **In-the-wild videos**: Videos with background frames, action repetitions, and varying action orderings (e.g., cooking recipes, assembly tasks)

### Key Innovation: GTCC (Generalized Temporal Cycle-Consistency)
The primary method, **GTCC**, extends traditional Temporal Cycle-Consistency (TCC) to handle:
1. **Repeated actions**: Using Gaussian Mixture Models (GMM) to find multiple soft nearest neighbors
2. **Background/droppable frames**: Using a context-dependent drop function
3. **Cross-task learning**: Using Multi-head Crosstask Network (MCN)

### Main Insight
Progress prediction emerges as a useful **byproduct** of learning a multi-cycleback consistent embedding space among same-activity videos that are alignable.

---

## 2. Project Structure

```
/vision/anishn/GTCC_CVPR2024/
├── README.md                          # Documentation with run commands
├── Video Alignment.pdf                # CVPR 2024 paper
├── configs/                           # Configuration management
│   ├── entry_config.py               # Entry point for config parsing
│   └── generic_config.py             # Base configuration dictionary
├── models/                            # Model architectures
│   ├── model_singleprong.py          # Single-prong encoders (ResNet50, etc.)
│   ├── model_multiprong.py           # Multi-Prong Attention Network (MCN)
│   ├── json_dataset.py               # PyTorch Dataset implementation
│   ├── alignment_training_loop.py    # Training loop
│   └── protas_model.py               # ProTAS model (action/progress)
├── utils/                             # Utility functions
│   ├── loss_functions.py             # GTCC, TCC, LAV, VAVA losses
│   ├── evaluation.py                 # Evaluation metrics (OPE, PC, EAE, KT)
│   ├── tensorops.py                  # Core tensor operations & GMM
│   └── ...                           # Other utilities
├── external_util/                     # External algorithms
│   ├── softdtw.py                    # Soft Dynamic Time Warping
│   └── ot_pytorch.py                 # Optimal Transport (Sinkhorn)
├── dset_jsons/                        # Dataset metadata JSONs
├── singletask_train.py               # Single-task training
├── multitask_train.py                # Multi-task training
└── eval.py                           # Evaluation script
```

---

## 3. Correct Run Commands (from README)

### Environment Setup
```bash
# Python 3.8.10, Ubuntu 18.04 recommended
pip install -r ./requirements-py3.8.10.txt

# Set environment variables in ~/.bashrc
export DATASET_PATH=<PATH_TO_DATASET_FOLDER>
export OUTPUT_PATH='./output'
export JSON_DPATH='./dset_jsons'
```

### Single-Task Training (one model per task)
```bash
python singletask_train.py 1 --gtcc --ego --resnet
python singletask_train.py 1 --vava --ego --resnet
python singletask_train.py 1 --lav --ego --resnet
python singletask_train.py 1 --tcc --ego --resnet
```

### Cross-Task/Multi-Task Training (one model for all tasks)
```bash
python multitask_train.py 1 --gtcc --ego --resnet --mcn
python multitask_train.py 1 --vava --ego --resnet --mcn
python multitask_train.py 1 --lav --ego --resnet --mcn
python multitask_train.py 1 --tcc --ego --resnet --mcn
```

### Evaluation
```bash
python eval.py -f output/single-task-setting/V1___GTCC_egoprocel
python eval.py -f output/multi-task-setting/V1___GTCC_egoprocel
```

---

## 4. Online Progress Error (OPE) - Main Focus Area

### What is OPE?
**Online Progress Error (OPE)** is the primary evaluation metric introduced in this paper for measuring progress prediction quality. It computes the **mean absolute difference** between predicted progress and ground-truth progress.

### Mathematical Definition

From the paper (Equation 12), the **geodesic progress** at frame `u_i` is:

```
π̂(u_i) = Σ_{t=1}^{i-1} ||u_{t+1} - u_t|| / Ĝ
```

Where:
- `||u_{t+1} - u_t||` is the Euclidean distance between consecutive frame embeddings
- `Ĝ` is the estimated total geodesic distance (averaged from training videos)

**OPE** is then defined as:
```
OPE = mean(|π̂(u_i) - π_gt(u_i)|) for all frames
```

### Code Location
**File**: `utils/evaluation.py` (lines 222-307)
**Class**: `OnlineGeoProgressError`

### Key Implementation Details

#### 1. Ground Truth Progress Calculation (`get_trueprogress`)
**File**: `utils/tensorops.py` (lines 298-310)

```python
def get_trueprogress(time_dict):
    N = time_dict['end_frame'][-1] + 1
    progress = torch.zeros(N)
    prev_prg = 0
    prg = 1 / sum([1 if step != 'SIL' else 0 for step in time_dict['step']])
    for step, start, end in zip(time_dict['step'], time_dict['start_frame'], time_dict['end_frame']):
        if step != 'SIL':
            progress[start:end+1] = torch.linspace(prev_prg, prev_prg + prg, round(end - start + 1))
            prev_prg = prev_prg + prg
        else:
            progress[start:end+1] = progress[start-1]  # SIL frames don't contribute
    return progress
```

**Key assumptions**:
- Each action segment contributes **equally** to overall progress (1/num_actions)
- Progress increases **linearly** within each action segment
- **SIL (silence/background)** frames do NOT contribute to progress - progress stays constant

#### 2. Predicted Progress via Geodesic Distance (`get_cum_matrix`)
**File**: `utils/tensorops.py` (lines 327-331)

```python
def get_cum_matrix(video):
    P = torch.zeros(video.shape[0])
    for t in range(1, video.shape[0]):
        P[t] = P[t-1] + torch.linalg.norm(video[t] - video[t-1])
    return P
```

This computes **cumulative geodesic distance** - the sum of inter-frame Euclidean distances in the embedding space.

#### 3. Normalization Factor (`get_average_train_cum_distance`)
**File**: `utils/tensorops.py` (lines 264-295)

Since we don't have access to future frames at inference time, `Ĝ` is estimated by:
- Computing total geodesic distance for all training videos of the same task
- Taking the **mean** as the normalization factor

```python
def get_average_train_cum_distance(model, testfolder, data_structure, targ_task=None, skip_rate=None):
    # ... loads training data
    for task in tasks:
        for inputs, times in train_dls[task]:
            outputs = model(inputs)['outputs']
            cum_dists = []
            for output in outputs:
                cum_total = get_cum_matrix(output).max()  # Total geodesic distance
                cum_dists.append(cum_total.item())
            means[task] = np.mean(cum_dists)
            vars[task] = np.var(cum_dists)
    return means, vars
```

#### 4. OPE Computation Flow
**File**: `utils/evaluation.py` - `OnlineGeoProgressError.__call__`

```python
def __call__(self, model, config_obj, epoch, test_dataloaders, testfolder, tasks, ...):
    for task in tasks:
        # Step 1: Get normalization factor from training videos
        train_cum_means, train_cum_vars = get_average_train_cum_distance(...)

        gpe_list = []
        embedded_dl = flatten_dataloader_and_get_dict(model, test_dataloaders[task], ...)

        for i, (outputs_dict, tdict) in enumerate(embedded_dl):
            outputs = outputs_dict['outputs']

            # Step 2: Compute ground truth progress
            true_progress = get_trueprogress(tdict)

            # Step 3: Compute predicted progress via geodesic distance
            pred2_progress = get_cum_matrix(outputs)
            pred2_progress = pred2_progress / train_cum_means[task]  # Normalize

            # Step 4: Compute mean absolute error
            gpe = torch.mean(torch.abs(true_progress - pred2_progress))
            gpe_list.append(gpe.item())

        return {'task': task, 'ogpe': np.mean(gpe_list), 'CoV': train_cum_vars[task] / train_cum_means[task]}
```

### Why OPE is Important

1. **Online Evaluation**: Unlike other metrics, OPE works in an **online setting** - it only uses frames up to the current time step, making it applicable for real-time AR/robotics applications.

2. **No Monotonicity Assumption**: OPE works for both monotonic and in-the-wild videos (unlike PHP which assumes monotonicity).

3. **Direct Progress Measurement**: While PC/EAE measure alignment quality, OPE directly measures what we care about - **progress prediction accuracy**.

4. **Practical Applications**: Progress estimation is crucial for:
   - AR assistant glasses showing step information
   - Action segmentation/anticipation
   - Anomaly detection (decrease in progress = error)

### OPE Results from Paper

| Dataset | TCC | LAV | VAVA | **GTCC** |
|---------|-----|-----|------|----------|
| Pouring | 38.9 | 31.4 | 37.8 | **22.5** |
| Penn Action | 35.5 | 25.5 | 80.5 | **14.5** |
| EgoProceL | 34.0 | 36.2 | 35.5 | **13.6** |
| COIN | 28.7 | 22.7 | 27.6 | **21.4** |

GTCC significantly outperforms all baselines on OPE, especially on in-the-wild datasets.

---

## 5. GTCC Loss Function - Technical Details

### The Problem with TCC
Standard TCC assumes:
1. Each action appears only once (unimodal distribution)
2. All frames should be aligned (no background handling)

### GTCC Solution

#### 5.1 Multi-Neighbor Cycle Consistency (for repeated actions)

Instead of finding a single Soft Nearest Neighbor (SNN), GTCC finds **K SNNs** using GMM:

```python
# From the paper (Equation 4-5):
# α_j ≈ p_j = Σ_{k=1}^{K} ψ_k * p_j^{(k)}

# Each SNN is computed as:
# ṽ^(k) = Σ_{j=1}^{M} p_j^{(k)} * v_j
```

The GMM fitting is implemented in `utils/tensorops.py:get_gmm_lfbgf()`:
- Uses L-BFGS-B optimization
- Minimizes KL divergence between α (forward distribution) and GMM approximation
- Returns K discrete Gaussians with weights ψ_k

#### 5.2 Droppable Frame Detection (for background frames)

GTCC learns a context-dependent drop function:

```python
# From the paper (Equation 9-10):
# c(V) = (1/M) * Σ_{j=1}^{M} c(v_j)  # Context from video V
# P_drop(u_i|V) = sigmoid([u_i^T, 1] * c(V))  # Drop probability
```

#### 5.3 Final GTCC Loss (Equation 11)

```python
L = (1 - P_drop(u_i|V)) * L_multi-cbr + P_drop(u_i|V) / L_multi-cbr
```

- **Alignable frames** (P_drop ≈ 0): Minimize cycle-back regression loss
- **Droppable frames** (P_drop ≈ 1): Maximize cycle-back regression loss (bad alignment)

---

## 6. Multi-head Crosstask Network (MCN)

### Architecture
```
Input Video S
      ↓
[Base Network] → U^b (shared features)
      ↓
┌─────┼─────┐
↓     ↓     ↓
Head1 Head2 ... HeadL → {U^h_1, U^h_2, ..., U^h_L}
      ↓
[Multi-head Attention] → U (final encoding)
```

### Purpose
- Learn shared information across related tasks
- Automatically select relevant heads via attention
- **No need to know task label at inference time**

### Results (from paper Table 4)
Using MCN consistently improves:
- EAE: 100% of experiments
- KT/KTW: 100% of experiments
- OPE: 78% of experiments

---

## 7. Other Evaluation Metrics

| Metric | Description | Higher/Lower Better |
|--------|-------------|---------------------|
| **OPE** | Online Progress Error - main metric | Lower ↓ |
| **PC** | Phase Classification - SVM accuracy on action labels | Higher ↑ |
| **EAE** | Enclosed Area Error - alignment quality | Lower ↓ |
| **KT** | Kendall's Tau - temporal correlation (monotonic) | Higher ↑ |
| **KTW** | Kendall's Tau in the Wild (handles non-monotonic) | Higher ↑ |
| **PHP** | Phase Progression - R² of linear regressor | Higher ↑ |

---

## 8. Datasets

| Dataset | Type | Characteristics |
|---------|------|-----------------|
| **Pouring** | Monotonic | Humans pouring liquid into cups |
| **Penn Action** | Monotonic | Athletic maneuvers (tennis, jumping) |
| **COIN** | In-the-wild | Instructional videos (180 tasks) |
| **EgoProceL** | In-the-wild | Egocentric procedural videos (CMU-MMAC, EGTEA, MECCANO, EPIC-tent) |

---

## 9. Key Configuration Parameters

### GTCC-Specific Parameters
```python
GTCC_PARAMS = {
    'softmax_temp': 0.1,      # Temperature for softmax in α computation
    'max_gmm_iters': 8,       # GMM optimization iterations
    'n_components': K,        # Number of GMM components (5 monotonic, 15 in-the-wild)
    'delta': δ,               # Margin proportion (0.05 in-the-wild, 0.10 monotonic)
    'gamma': γ,               # Dropout decay parameter
}
```

### Default Values by Dataset
| Dataset | K (n_components) | δ (delta) | γ (gamma) |
|---------|-----------------|-----------|-----------|
| egoprocel/cmu/egtea | 15 | 0.05 | 0.05 |
| penn-action/pouring | 5 | 0.10 | 0.10 |

---

## 10. Summary

### Key Contributions
1. **GTCC Loss**: Generalizes TCC to handle repeated actions (via GMM) and background frames (via drop function)
2. **MCN Architecture**: Multi-head attention network for cross-task learning without requiring task labels
3. **OPE Metric**: Online progress evaluation that works for in-the-wild videos

### Main Results
- GTCC significantly outperforms TCC, LAV, and VAVA on **OPE** (the main metric)
- Especially effective on **in-the-wild datasets** (EgoProceL, COIN)
- MCN improves multi-task performance across all methods

### Code Quality
- Well-structured codebase with clear separation of concerns
- Supports distributed training (DDP)
- Comprehensive evaluation suite
- Memory-efficient implementation with gradient checkpointing

---

## 11. Citation

```bibtex
@InProceedings{Donahue_2024_CVPR,
    author    = {Donahue, Gerard and Elhamifar, Ehsan},
    title     = {Learning to Predict Activity Progress by Self-Supervised Video Alignment},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {18667-18677}
}
```

---

## 12. Contact
For questions: donahue [DOT] g [AT] northeastern [DOT] edu
